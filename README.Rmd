---
title: "Project 2"
author: "Brian Sugg"
date: "7/3/2020"
output:
  rmarkdown::github_document:
    toc: true
    toc_depth: 3
params:
  day: "Monday"
---

```{r setup, include=FALSE}
library(tidyverse)
library(Lahman)
library(formatR)
library(dplyr)
library(class)
library(caret)
library(e1071)
library(randomForest)
library(gbm)
knitr::opts_chunk$set(echo=TRUE,tidy.opts=list(width.cutoff=70),tidy=TRUE)
```

# Introduction  

## Purpose  

The overall theme of this exercise is determining the popularity of online news. The goal is to create models for predicting the number of times a news article from *mashable.com* will be shared in social networks. Two models will be created: a linear regression model and a non-linear model. The parameter functionality of markdown will be used to automatically generate an analysis report for each day of the week that an article might be published.  

## Data Description  

The data comes from the *UC Irvine Machine Learning Repository* and can be found in its original form at this [link](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#).  

This data set covers a two year period, listing all published articles from Mashable and several variables of associated characteristics, such as:  

*  Published Day of Week
*  Channel, or Genre (Lifestyle, Business, World, Entertainment, etc.)
*  Word Count
*  Number of Images
*  *...and several others...*

In total there are **58 possible predictive attributes** with the goal of predicting the **response variable of number of shares in social networks**. The provided link above goes into more detail for all variables and their representation.  

## *****Methods  

**To be filled in later** >> The methods you’ll use (roughly - more detail can be given later in the document).  

# Data  

## Metadata  

As mentioned previously, the original data set contains 58 possible predictor variables and 1 response variable. We will add 1 additional response variable for "Popularity" during the import process below in case this is something we wish to also predict. *Popularity* will be defined as any article that is shared at least 1,400 times.

The provided predictor variables have a wide range of characteristics they represent. A listing of some of the primary variables this analysis will focus on, including the thought process behind their selection:  

*  `n_tokens_title` - number of words in the title  
    + Are short titles more catchy to the reader?  
*  `n_tokens_content` - number of words in the article  
    + Is a shorter article more likely to be quickly digested and shared vs longer articles that require more time?  
*  `num_imgs` - number of images in the article  
    + Are more images more visually appealing, generating shares from people wanting to share the images within?  
*  `num_videos` - number of videos in the article  
    + Videos can provide article content more easily, but also requires the attention of the viewer. Could this influence sharing?  
*  `data_channel_is_*` - 6 binary variables that indicate if an article falls under the genre *Lifestyle*, *Entertainment*, *Business*, *Social Media*, *Tech*, or *World*  
    + Does the channel/genre influence sharing?  

Many other variables are present, especially in determining the positive/negative association of words within the article and the title, which will be discussed further if they become prevalent in our analysis.  

## Import  

The raw data set from UCI is provided and read in as a `.csv` file, creating the `news` data set. Extreme values for `shares` > 25,000 are filtered out (578 removed out of 39,644 records). The parameter functionality of markdown is then incorporated from the YAML header, initially using the parameter value `day` to filter the `news` data set on a certain day of week for analysis.  

Additional modifications are made, including the creation of a binary `popularity` variable as mentioned earlier, defining popularity as any article shared more than 1,400 times. To help visualize the amount of articles per `channel`, this variable is created as well using the `data_channel_is*` variables. Finally, any non-predictive variables are removed to create a `newsSlice` data set that will be sliced into train and test sets later on.  

```{r createDataSet}
# Read in the data, filter out extreme values
news <- read_csv("data/OnlineNewsPopularity.csv")
news <- news %>% filter(shares<25000)
# Filter data set on day of week using markdown parameter
filterDay <- paste0("weekday_is_",tolower(params$day))
news <- filter(news,eval(as.symbol(filterDay))==1)
# Create a binary variable for popularity
news$sharesPopular <- factor(ifelse(news$shares<1400,0,1))
# Create variable for channel using the 6 individual channel variables if present
news$channel <- factor(ifelse(news$data_channel_is_lifestyle==1,"Lifestyle",ifelse(news$data_channel_is_entertainment==1,"Entertainment",ifelse(news$data_channel_is_bus==1,"Business",ifelse(news$data_channel_is_socmed==1,"Social Media",ifelse(news$data_channel_is_tech==1,"Tech",ifelse(news$data_channel_is_world==1,"World","Other")))))))
# Remove 2 non-predictive variables as noted in linked documentation from UCI
newsSlice <- news %>% select(-url,-timedelta)
```

For verification, a preview of the `newsSlice` set is generated prior to slicing:  

```{r dataPreview}
head(newsSlice)
```


## Slicing  

Utilizing the newly created `newsSlice` data set, a random sample is performed for 70% of the data to make up the training set `newsTrain`, and the remaining 30% to be used later as the test set `newsTest`. A seed is set beforehand to ensure reproducible sampling in the future if needed.  

```{r dataSlicing}
# Set seed for reproducible results
set.seed(1)
# Create the train and test sets
train <- sample(1:nrow(newsSlice),size=nrow(newsSlice)*0.7)
test <- dplyr::setdiff(1:nrow(newsSlice),train)
newsTrain <- newsSlice[train, ]
newsTest <- newsSlice[test, ]
```

For the analysis of articles published on **`r params$day`** there are a total of `r format(nrow(newsSlice),big.mark=",")` records sliced as:  

*  `r format(nrow(newsTrain),big.mark=",")` records used in the `newsTrain` data set (`r round(nrow(newsTrain)/nrow(newsSlice)*100,digits=2)`%)  
*  `r format(nrow(newsTest),big.mark=",")` records used in the `newsTest` data set (`r round(nrow(newsTest)/nrow(newsSlice)*100,digits=2)`%)  

# Summarizations  

Some basic summary statistics and visualizations of the `newsTrain` training data are provided in the below sections, using variables discussed previously.  

## Numeric Summaries  

A contingecy table is provided for the categorical variable `channel` to compare against article popularity, where a popular article is defined as having at least 1,400 shares.  

```{r contingencyTables}
# Contingency tables of frequency
knitr::kable(table(newsTrain$channel,newsTrain$sharesPopular),format="html",caption="Frequency of Article Channel by Sharing Popularity (0=NotPopular,1=Popular)")
```

Numeric summaries of different article stats help reveal a few observations from the training data set regarding the length of article titles, the length of articles, the number of images within an article, and the number of videos within an article. The correlation between these variables and the response variable `shares` will be explored further with visuals in the following section.  

```{r summaryStats}
knitr::kable(cbind("Title Word Count"=round(summary(newsTrain$n_tokens_title),1),"Article Word Count"=round(summary(newsTrain$n_tokens_content),1),"Number of Images"=round(summary(newsTrain$num_imgs),1),"Number of Videos"=round(summary(newsTrain$num_videos),1)),caption=paste("Summary of Article Characteristics"))
```

## Visuals  

A basic histogram is first explored for number of shares in the `newsTrain` data set, which is typically skewed right.  

```{r histogram}
# Histogram creation
histShares <- ggplot(data=newsTrain,aes(x=shares))
histShares + geom_histogram(bins=30) + labs(x="Number of Shares",y="Article Count",title="Histogram of Shares")
```

The number of shares can be better visualized in a boxplot by `channel`, showcasing the summary stats for each.  

```{r boxPlot}
# Boxplot creation by position
boxChannel <- ggplot(data=newsTrain)
boxChannel + geom_jitter(aes(x=channel,y=shares,color=channel)) + geom_boxplot(aes(x=channel,y=shares)) + labs(x="Article Channel",y="Shares on Social Networks",title="Boxplot for Number of Shares per Article Channel") + theme(legend.position = "none")
```

```{r scatterPlots}
# Scatter plot 1 creation
plotWordCount <- ggplot(data=newsTrain,aes(x=n_tokens_content,y=shares))
plotWordCount + geom_point() + geom_smooth(method=NULL) + labs(x="Article Word Count",y="Shares",title="Article Word Count vs Shares")
# Scatter plot 2 creation
plotImages <- ggplot(data=newsTrain,aes(x=num_imgs,y=shares))
plotImages + geom_point() + geom_smooth(method=NULL) + labs(x="Number of Images",y="Shares",title="Number of Images vs Shares")
# Scatter plot 3 creation
plotVideos <- ggplot(data=newsTrain,aes(x=n_tokens_content,y=shares))
plotVideos + geom_point() + geom_smooth(method=NULL) + labs(x="Number of Videos",y="Shares",title="Number of Videos vs Shares")
```

```{r bar100}
# 100% Stack bar chart on popularity
stackBar <- ggplot(data=newsTrain,aes(x=n_tokens_title))
stackBar + geom_bar(aes(fill=sharesPopular),position="fill") + labs(x="Title Word Count",y="Popularity %",title="Popularity (0=NotPopular,1=Popular) vs Title Word Count") + scale_fill_discrete(name="Popular Article")
```

A few plots help illustrate the above numeric summaries, and offer additional views.  

The general things that the plots describe should be explained but, since we are going to automate things, there is no need to try and explain particular trends in the plots you see (unless you want to try and automate that too!).  

# Modeling  

Following a similar method from the original use case presented by documentation on the UCI website, we will make our prediction challenge into a binary classification problem by dividing the `shares` variable into two groups to represent shares popularity, as created in the `sharesPopular` variable earlier. If the predicted value or `sharesPopular` is 1, then we anticipate at least 1,400 shares. Else, if the predicted value is 0 then we anticipate less than 1,400 shares.  

Two types of models will be fitted, tested, and analyzed for accuracy and misclassification of this prediction. As mentioned previously, the first will be an Ensemble Model via Random Forests and the second will be a Linear Regression model via a Generalized Linear Model for Logistic Regression. Both will test all possible predictor variables.  

The fit process will utilize the `caret` package and available relevant options for each model type, with more detail to follow. For each fit, we will rely on the functionality of the `caret` package to assist with k-fold cross validation, centering, and scaling.  

## Ensemble Model  

The approach with Random Forests includes building decision tress on bootstrapped training samples, then relying on a random sample of *m predictors* to be used as split candidates from the full set of provided predictors. This random sample of a small subset of predictors helps decorrelate the trees and prevent any strong predictors from consistently occuring in each tree. This differentiates Random Forests from Bagging, offering a slight improvement in prediction, which is why this method has been chosen in this exercise.

### Fit  

The fit for the Random Forests model is done here using k-fold cross validation on the `newsTrain` data set. Since fitting for this type of model is computationally expensive, only 5 folds have been chosen for cross validation, repeated 3 times. A seed has also been set to ensure reproducible results.  

Since our data set has been filtered on articles published for one specific day of the week, all `weekday_is_*` variables have been excluded from the training. Also, the data is centered and scaled using the `preProcess` option of the `train()` function from the `caret` package.  

```{r randomForestFit}
# 1. Use trainControl() function to control computations and set number of desired folds for cross validation
trctrl <- trainControl(method = "repeatedcv", number = 2, repeats = 3)
# 2. Set a seed for reproducible results
set.seed(3333)
# 3. Use train() function to determine a random forest model of best fit
randFor_fit <- train(sharesPopular ~ ., data = newsTrain[,c(1:29,38:58,60)], method = "rf", trControl=trctrl, preProcess = c("center", "scale"), tuneLength = 10)
```

### Selection  

```{r randomForestSelect}
# Print results with accuracy for best fit
randFor_fit
```

Given our training parameters discussed above, utilizing k-fold cross validaiton, our model selection is based on the provided training calculations output from the `caret` package, which indicates an optimal `MTRY` value of **`r randFor_fit$bestTune[[1]][[1]]`**. This model fit will be tested on `newsTest` data set and then measured for actual accuracy and associated misclassification rate.  

### Test Set Predictions  

The selected model is now applied to the `newsTest` data set, and a confusion matrix provided to detail the resulting accuracy of our *predictions vs the actual reference values* from the test set.  

```{r randomForestPredict}
# Create predictions of class variable on the test data set using our model
testPredRF <- predict(randFor_fit, newdata = newsTest)
# Generate confusion matrix showing table of results with accuracy
conMatrixRF <- confusionMatrix(testPredRF,newsTest$sharesPopular)
conMatrixRF
```

```{r randomForestMisRate}
# Use values from confusion matrix to calculate misclassification rate
misclassRateRF <- 1 - sum(diag(conMatrixRF$table))/sum(conMatrixRF$table)
```

Performance metrics for the **Random Forest** model predictions on `newsTest` with **`MTRY=``r randFor_fit$bestTune[[1]][[1]]`**:  
**Accuracy:** `r round(conMatrixRF$overall[1],4)`  
**Misclassification Rate:** `r round(misclassRateRF,4)`  

These values will be later compared against the upcoming Linear Regression Model to determine best performance between the two.  

## Linear Regression Model  

The approach for the Generalized Linear Model of Logistic Regression was chosen given the non-continuous, binary nature of our outcome of predicting either a 0 or 1 for popularity. The same set of possible predictors will be provided.  

### Fit  

The fit for the logistic regression model is done here again using k-fold cross validation on the `newsTrain` data set. Since less computation is required, we have increased our training to 10 folds, with resampling repeated 5 times. The `family="binomial"` argument has been provided to the `glm` method to explicitly state the desired fit of logistic regression, although the `caret` package should recognize this as the optimal approach even without the argument.  

As before, all `weekday_is_*` variables have been excluded from the training since the data set is filtered on just one published day of the week. The training data is still centered and scaled using the `preProcess` option of the `train()` function from the `caret` package.  

```{r logRegFit}
# 1. Use trainControl() function to control computations and set number of desired folds for cross validation
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
# 2. Set a seed for reproducible result
set.seed(3333)
# 3. Use train() function to determine a generalized linear regression model of best fit
logReg_fit <- train(sharesPopular ~ ., data = newsTrain[,c(1:29,38:58,60)], method = "glm", family="binomial", trControl=trctrl, preProcess = c("center", "scale"), tuneLength = 10)
```

### Selection  

```{r logRegSelect}
# Print results with accuracy for best fit
logReg_fit
```

Given our training parameters discussed above, utilizing k-fold cross validaiton, our model selection is based on the provided training calculations output from the `caret` package. This model fit will be tested on `newsTest` data set and then measured for actual accuracy and associated misclassification rate.  

### Test Set Predictions  

The selected model is now applied to the `newsTest` data set, and a confusion matrix provided to detail the resulting accuracy of our *predictions vs the actual reference values* from the test set.

```{r logRegPredict}
# Create predictions of class variable on the test data set using our model
testPredGLM <- predict(logReg_fit, newdata = newsTest)
# Generate confusion matrix showing table of results with accuracy
conMatrixGLM <- confusionMatrix(testPredGLM,newsTest$sharesPopular)
conMatrixGLM
```

```{r logRegMisRate}
# Use values from confusion matrix to calculate misclassification rate
misclassRateGLM <- 1 - sum(diag(conMatrixGLM$table))/sum(conMatrixGLM$table)
```

Performance metrics for the **Generalized Linear Regression** model predictions on `newsTest`:  
**Accuracy:** `r round(conMatrixGLM$overall[1],4)`  
**Misclassification Rate:** `r round(misclassRateGLM,4)`  

These values will be compared in the upcoming *Conclusion* section to determine the best performance between the two models.  

# *************************Automation  

Once you’ve completed the above for Monday, adapt the code so that you can use a parameter in your build process that will cycle through the weekday_is_* variables.  

The analysis for [Monday is available here](MondayAnalysis.md)

# *************************Conclusion

No description given. Explain final model selection and evaluation of accuracy and misclassification rate metrics.  

