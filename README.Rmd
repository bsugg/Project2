---
title: "Project 2"
author: "Brian Sugg"
date: "7/3/2020"
output:
  rmarkdown::github_document:
    toc: true
    toc_depth: 3
params:
  day: "Monday"
---

```{r setup, include=FALSE}
library(tidyverse)
library(Lahman)
library(formatR)
library(dplyr)
library(class)
library(caret)
library(e1071)
library(randomForest)
library(gbm)
knitr::opts_chunk$set(echo=TRUE,tidy.opts=list(width.cutoff=70),tidy=TRUE)
```

# Introduction  

## Purpose  

The overall theme of this exercise is determining the popularity of online news. The goal is to create models for predicting the number of times a news article from *mashable.com* will be shared in social networks. Two models will be created: a linear regression model and a non-linear model. The parameter functionality of markdown will be used to automatically generate an analysis report for each day of the week that an article might be published.  

## Data Description  

The data comes from the *UC Irvine Machine Learning Repository* and can be found in its original form at this [link](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#).  

This data set covers a two year period, listing all published articles from Mashable and several variables of associated characteristics, such as:  

*  Published Day of Week
*  Channel, or Genre (Lifestyle, Business, World, Entertainment, etc.)
*  Word Count
*  Number of Images
*  *...and several others...*

In total there are **58 possible predictive attributes** with the goal of predicting the **response variable of number of shares in social networks**. The provided link above goes into more detail for all variables and their representation.  

## *****Methods  

**To be filled in later** >> The methods you’ll use (roughly - more detail can be given later in the document).  

# Data  

## Metadata  

As mentioned previously, the original data set contains 58 possible predictor variables and 1 response variable. We will add 1 additional response variable for "Popularity" during the import process below in case this is something we wish to also predict. *Popularity* will be defined as any article that is shared at least 1,400 times.

The provided predictor variables have a wide range of characteristics they represent. A listing of some of the primary variables this analysis will focus on, including the thought process behind their selection:  

*  `n_tokens_title` - number of words in the title  
    + Are short titles more catchy to the reader?  
*  `n_tokens_content` - number of words in the article  
    + Is a shorter article more likely to be quickly digested and shared vs longer articles that require more time?  
*  `num_imgs` - number of images in the article  
    + Are more images more visually appealing, generating shares from people wanting to share the images within?  
*  `num_videos` - number of videos in the article  
    + Videos can provide article content more easily, but also requires the attention of the viewer. Could this influence sharing?  
*  `data_channel_is_*` - 6 binary variables that indicate if an article falls under the genre *Lifestyle*, *Entertainment*, *Business*, *Social Media*, *Tech*, or *World*  
    + Does the channel/genre influence sharing?  

Many other variables are present, especially in determining the positive/negative association of words within the article and the title, which will be discussed further if they become prevalent in our analysis.  

## Import  

The raw data set from UCI is provided and read in as a `.csv` file, creating the `news` data set. Extreme values for `shares` > 25,000 are filtered out (578 removed out of 39,644 records). The parameter functionality of markdown is then incorporated from the YAML header, initially using the parameter value `day` to filter the `news` data set on a certain day of week for analysis.  

Additional modifications are made, including the creation of a binary `popularity` variable as mentioned earlier, defining popularity as any article shared more than 1,400 times. To help visualize the amount of articles per `channel`, this variable is created as well using the `data_channel_is*` variables. Finally, any non-predictive variables are removed to create a `newsSlice` data set that will be sliced into train and test sets later on.  

```{r createDataSet}
# Read in the data, filter out extreme values
news <- read_csv("data/OnlineNewsPopularity.csv")
news <- news %>% filter(shares<25000)
# Filter data set on day of week using markdown parameter
filterDay <- paste0("weekday_is_",tolower(params$day))
news <- filter(news,eval(as.symbol(filterDay))==1)
# Create a binary variable for popularity
news$sharesPopular <- factor(ifelse(news$shares<1400,0,1))
# Create variable for channel using the 6 individual channel variables if present
news$channel <- factor(ifelse(news$data_channel_is_lifestyle==1,"Lifestyle",ifelse(news$data_channel_is_entertainment==1,"Entertainment",ifelse(news$data_channel_is_bus==1,"Business",ifelse(news$data_channel_is_socmed==1,"Social Media",ifelse(news$data_channel_is_tech==1,"Tech",ifelse(news$data_channel_is_world==1,"World","Other")))))))
# Remove 2 non-predictive variables as noted in linked documentation from UCI
newsSlice <- news %>% select(-url,-timedelta)
```

For verification, a preview of the `newsSlice` set is generated prior to slicing:  

```{r dataPreview}
head(newsSlice)
```


## Slicing  

Utilizing the newly created `newsSlice` data set, a random sample is performed for 70% of the data to make up the training set `newsTrain`, and the remaining 30% to be used later as the test set `newsTest`. A seed is set beforehand to ensure reproducible sampling in the future if needed.  

```{r dataSlicing}
# Set seed for reproducible results
set.seed(1)
# Create the train and test sets
train <- sample(1:nrow(newsSlice),size=nrow(newsSlice)*0.7)
test <- dplyr::setdiff(1:nrow(newsSlice),train)
newsTrain <- newsSlice[train, ]
newsTest <- newsSlice[test, ]
```

For the analysis of articles published on **`r params$day`** there are a total of `r format(nrow(newsSlice),big.mark=",")` records sliced as:  

*  `r format(nrow(newsTrain),big.mark=",")` records used in the `newsTrain` data set (`r round(nrow(newsTrain)/nrow(newsSlice)*100,digits=2)`%)  
*  `r format(nrow(newsTest),big.mark=",")` records used in the `newsTest` data set (`r round(nrow(newsTest)/nrow(newsSlice)*100,digits=2)`%)  

# Summarizations  

Some basic summary statistics and visualizations of the `newsTrain` training data are provided in the below sections, using variables discussed previously.  

## Numeric Summaries  

A contingecy table is provided for the categorical variable `channel` to compare against article popularity, where a popular article is defined as having at least 1,400 shares.  

```{r contingencyTables}
# Contingency tables of frequency
knitr::kable(table(newsTrain$channel,newsTrain$sharesPopular),format="html",caption="Frequency of Article Channel by Sharing Popularity (0=NotPopular,1=Popular)")
```

Numeric summaries of different article stats help reveal a few observations from the training data set regarding the length of article titles, the length of articles, the number of images within an article, and the number of videos within an article. The correlation between these variables and the response variable `shares` will be explored further with visuals in the following section.  

```{r summaryStats}
knitr::kable(cbind("Title Word Count"=round(summary(newsTrain$n_tokens_title),1),"Article Word Count"=round(summary(newsTrain$n_tokens_content),1),"Number of Images"=round(summary(newsTrain$num_imgs),1),"Number of Videos"=round(summary(newsTrain$num_videos),1)),caption=paste("Summary of Article Characteristics"))
```


## Visuals  

```{r boxPlot}
# Boxplot 1 creation by position
boxChannel <- ggplot(data=newsTrain)
boxChannel + geom_jitter(aes(x=channel,y=shares,color=channel)) + geom_boxplot(aes(x=channel,y=shares)) + labs(x="Article Channel",y="Shares on Social Networks",title="Boxplot for Number of Shares per Article Channel") + theme(legend.position = "none")
```


```{r scatterPlot}
# Filter out a few outliers to help in scatter plot
newsTrain <- filter(newsTrain,n_tokens_content<1500)
newsTrain <- filter(newsTrain,n_tokens_content>0)
newsTrain <- filter(newsTrain,shares<25000)

plot(newsTrain$n_tokens_title,newsTrain$shares)

plot(newsTrain$n_tokens_content,newsTrain$shares)

plot(newsTrain$num_imgs,newsTrain$shares)

plot(newsTrain$num_videos,newsTrain$shares)
```


A few plots help illustrate the above numeric summaries, and offer additional views.  

The general things that the plots describe should be explained but, since we are going to automate things, there is no need to try and explain particular trends in the plots you see (unless you want to try and automate that too!).  

# *************************Modeling  

Once you have your training data set, we are ready to fit some models. You should fit two types of models to predict the shares. One model should be an ensemble model (bagged trees, random forests, or boosted trees) and one should be a linear regression model (or collection of them that you’ll choose from).  

The article referenced in the UCI website mentions that they made the problem into a binary classification problem by dividing the shares into two groups (< 1400 and ≥ 1400), you can do this if you’d like or simply
try to predict the shares themselves.  

Feel free to use code similar to the notes or use the caret package.

## **********Ensemble Model  

### Fit  

Fit model on training data set.  

### Selection  

Your methodology for choosing your model during the training phase should be explained.  

### Test Set Predictions  

After training/tuning your two types of models (linear and non-linear) using cross-validation, AIC, or your preferred method (all on the training data set only!) you should then compare them on the test set.  

Include metrics for accuracy and misclassification rate.  

## **********Linear Regression Model  

### Fit  

Fit model on training data set.  

### Selection  

Your methodology for choosing your model during the training phase should be explained.  

### Test Set Predictions  

After training/tuning your two types of models (linear and non-linear) using cross-validation, AIC, or your preferred method (all on the training data set only!) you should then compare them on the test set.  

Include metrics for accuracy and misclassification rate.  

# *************************Automation  

Once you’ve completed the above for Monday, adapt the code so that you can use a parameter in your build process that will cycle through the weekday_is_* variables.  

The analysis for [Monday is available here](MondayAnalysis.md)

# *************************Conclusion

No description given. Explain final model selection and evaluation of accuracy and misclassification rate metrics.  

